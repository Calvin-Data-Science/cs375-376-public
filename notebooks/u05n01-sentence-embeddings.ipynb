{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Embeddings\n",
        "\n",
        "We'll see how we can represent *sentences* using *vectors* in a high-dimensional space, and how we measure and visualize *similarity* in that space.\n",
        "\n",
        "\n",
        "Example based on https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py\n"
      ],
      "metadata": {
        "id": "JdFT118Pw-4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import\n",
        "\n",
        "Press the Run button below (next to \"3 cells hidden\")"
      ],
      "metadata": {
        "id": "AGbR8zD7Z18I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsoFJqXBufv-"
      },
      "outputs": [],
      "source": [
        "# Install the needed libraries\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -U tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up TensorBoard to view the embeddings.\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "QCaTOyTtwKtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries we'll need\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "import csv\n",
        "import time"
      ],
      "metadata": {
        "id": "8CNqejviuheA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and Data\n",
        "\n",
        "In this example, we download a large set of questions from Quora and then find similar questions in this set.\n",
        "\n",
        "**Press the Run button below**."
      ],
      "metadata": {
        "id": "krRcOErzaMin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model for computing sentence embeddings. We use one trained for similar questions detection\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "n6Y34lviaHnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We download the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n",
        "# and find similar questions in it\n",
        "url = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n",
        "dataset_path = \"quora_duplicate_questions.tsv\"\n",
        "max_corpus_size = 5000 # We limit our corpus to only the first 5k questions\n",
        "\n",
        "\n",
        "# Check if the dataset exists. If not, download and extract\n",
        "# Download dataset if needed\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Download dataset\")\n",
        "    util.http_get(url, dataset_path)\n",
        "\n",
        "# Get all unique sentences from the file\n",
        "corpus_sentences = set()\n",
        "with open(dataset_path, encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row in reader:\n",
        "        corpus_sentences.add(row['question1'])\n",
        "        corpus_sentences.add(row['question2'])\n",
        "        if len(corpus_sentences) >= max_corpus_size:\n",
        "            break\n",
        "corpus_sentences = list(corpus_sentences)\n"
      ],
      "metadata": {
        "id": "_eSAnczpusFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Sentence Vectors"
      ],
      "metadata": {
        "id": "2ZAM7XCqbs4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tell the model to compute the *embeddings* for each sentence. This will take about a minute."
      ],
      "metadata": {
        "id": "7uf2p_9icA3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sentences[:3]"
      ],
      "metadata": {
        "id": "JUmk92XJnXQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus_sentences)"
      ],
      "metadata": {
        "id": "QQNRDxAQniT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "ErP5m9F9uu2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings[2].shape"
      ],
      "metadata": {
        "id": "OSg-PWqEnfjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Sentence Vectors"
      ],
      "metadata": {
        "id": "Sx7d7Q722gaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the two cells below to launch a viewer to show these embeddings.\n",
        "\n",
        "**Switch to UMAP mode.** (bottom left pane)\n",
        "\n",
        "Try rotating the view by dragging. Notice that some points that appeared on top of each other actually were in different places but just looked nearby because we were taking a 2D picture of a 3D space. By analogy, even the 3D space is a picture of a much higher dimensional space (384 dimensions in this case).\n",
        "\n",
        "Rotate the view around until you can clearly see a clump of points that isn't overlapped with some other points. It's easiest to see these on the outside edges of the \"ball\" of data. Mouse around that clump to see what the sentences are. **Try to identify a characteristic that those sentences have in common.** Also think about what's different among those sentences: what does the embedding projection *not* capture?\n",
        "\n",
        "Next, try clicking on an individual sentence. Look on the right pane: this is \"getting a tape measure out\" and looking at distances (or similarities) in the original space.\n"
      ],
      "metadata": {
        "id": "C3-AMsEyxijh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the embeddings to a file so that the projector can view them.\n",
        "writer = SummaryWriter()\n",
        "writer.add_embedding(corpus_embeddings, corpus_sentences)\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "Zir4Pdy1wX88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!tensorboard dev upload --logdir=/content/runs --verbose 1 --plugins projector --one_shot --name=\"Example SentenceTransformer Embeddings\" --description=\"Sample from Quora Duplicate Questions, see https://gist.github.com/kcarnold/956889ff81c1fd2fca8320ad86f23388\"\n",
        "# fails with: Error [from server]: The plugin \"projector\" is not currently supported in TensorBoard.dev. Supported plugins: [scalars,graphs,histograms,hparams,text]."
      ],
      "metadata": {
        "id": "6OprFKNQMsF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "-Qr8NWiGsQmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Clusters\n",
        "\n",
        "The approach we'll use here looks for \"communities\" of sentences. It tries to find groups of highly-similar sentences. It doesn't try to assign *every* sentence to a community.\n",
        "\n",
        "There are two parameters that we can configure:\n",
        "\n",
        "1. How similar do sentences need to be? If a sentence isn't similar enough to a community, it won't get included.\n",
        "2. How big do communities need to be? If a community is too small, it won't get reported."
      ],
      "metadata": {
        "id": "7Giidd0Cbmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Two parameters to tune:\n",
        "# min_cluster_size: Only consider cluster that have at least a certain number of sentences\n",
        "# threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
        "clusters = util.community_detection(corpus_embeddings, min_community_size=5, threshold=0.75)\n",
        "\n",
        "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n",
        "\n",
        "# Print for all clusters the top 3 and bottom 3 elements\n",
        "for i, cluster in enumerate(clusters):\n",
        "    print(\"\\nCommunity {} ({} sentences)\".format(i+1, len(cluster)))\n",
        "    for sentence_id in cluster[0:3]:\n",
        "        print(\"\\t\", corpus_sentences[sentence_id])\n",
        "    print(\"\\t\", \"...\")\n",
        "    for sentence_id in cluster[-3:]:\n",
        "        print(\"\\t\", corpus_sentences[sentence_id])"
      ],
      "metadata": {
        "id": "Zbi63WFcvJQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does it work?\n",
        "\n",
        "Now we have a *vector* for each sentence (in this case, each question). They are stored in an object called a *tensor*. Each row of the tensor corresponds to a sentence. The elements in that row are the vector for that sentence."
      ],
      "metadata": {
        "id": "LKIQ8zO9diLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings"
      ],
      "metadata": {
        "id": "z5pYfUUTvmOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings.shape"
      ],
      "metadata": {
        "id": "3obQDYO8dvPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can get out the vector for a sentence."
      ],
      "metadata": {
        "id": "UWaoX3jUd1aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look for a few example sentences by keywords\n",
        "gmail_sents = [(i, sent) for i, sent in enumerate(corpus_sentences) if 'password' in sent and 'gmail' in sent.lower()]\n",
        "gmail_sents"
      ],
      "metadata": {
        "id": "74-z-nj0vtM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_idx = gmail_sents[0][0]\n",
        "print(\"Getting the vector for sentence {}: \\\"{}\\\"\".format(sentence_idx, corpus_sentences[sentence_idx]))\n",
        "vec = corpus_embeddings[sentence_idx]\n",
        "\n",
        "print(\"The vector has\", len(vec), \"elements.\")"
      ],
      "metadata": {
        "id": "A5dUjdiZ9Vs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking for Similar Vectors\n",
        "\n",
        "Let's try to compute the similarity of this vector with every other vector. We do this by multiplying corresponding elements of the two vectors and adding up the result. (This is called a *dot product*.) It turns out that we can do this with a *matrix multiplication*."
      ],
      "metadata": {
        "id": "EYZv7C8kegli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_scores = corpus_embeddings.matmul(vec)\n",
        "similarity_scores"
      ],
      "metadata": {
        "id": "BQn3l4o2wVf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which sentences have vectors that are *most* similar? Let's look at which sentences correspond to the top `k` most similar vectors."
      ],
      "metadata": {
        "id": "W1K6A7-SfNiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[corpus_sentences[i] for i in similarity_scores.topk(15).indices]"
      ],
      "metadata": {
        "id": "GenIRe4ZfKC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0WsYZUHM5ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}