{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-09T21:07:34.746996Z",
     "iopub.status.busy": "2024-04-09T21:07:34.746521Z",
     "iopub.status.idle": "2024-04-09T21:07:52.907046Z",
     "shell.execute_reply": "2024-04-09T21:07:52.906074Z",
     "shell.execute_reply.started": "2024-04-09T21:07:34.746967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.0\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Setup the environment\n",
    "#!pip install --upgrade huggingface_hub transformers\n",
    "!pip install bitsandbytes\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "access_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = access_token_read)\n",
    "#!pip install git+https://github.com/huggingface/transformers -U\n",
    "#!pip install accelerate\n",
    "#!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-10T12:06:11.539116Z",
     "iopub.status.busy": "2024-04-10T12:06:11.538756Z",
     "iopub.status.idle": "2024-04-10T12:07:28.563453Z",
     "shell.execute_reply": "2024-04-10T12:07:28.562619Z",
     "shell.execute_reply.started": "2024-04-10T12:06:11.539047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436559d2fe94419289897ab673303060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 12:07:19.792633: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 12:07:19.792768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 12:07:19.922504: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Work around a bug in the version of PyTorch and GPU hardware curretnly on Kaggle. On other hardware, removing these lines may lead to a speed-up.\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Load the model\n",
    "USE_INSTRUCTION_TUNED = True\n",
    "if USE_INSTRUCTION_TUNED:\n",
    "    model_name = '/kaggle/input/gemma/transformers/1.1-2b-it/1'\n",
    "    if not os.path.exists(model_name):\n",
    "        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n",
    "        model_name = \"google/gemma-1.1-2b-it\"\n",
    "else:\n",
    "    model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n",
    "    if not os.path.exists(model_name):\n",
    "        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n",
    "        model_name = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "# Silence a warning.\n",
    "tokenizer.decode([tokenizer.eos_token_id]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:42:53.091769Z",
     "iopub.status.busy": "2024-04-10T11:42:53.091403Z",
     "iopub.status.idle": "2024-04-10T11:42:53.097809Z",
     "shell.execute_reply": "2024-04-10T11:42:53.096895Z",
     "shell.execute_reply.started": "2024-04-10T11:42:53.091734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check where the whole model is loaded and what data type it's using.\n",
    "model.device, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T21:12:05.348612Z",
     "iopub.status.busy": "2024-04-09T21:12:05.348261Z",
     "iopub.status.idle": "2024-04-09T21:12:05.420171Z",
     "shell.execute_reply": "2024-04-09T21:12:05.419141Z",
     "shell.execute_reply.started": "2024-04-09T21:12:05.348571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check where parameters are loaded. If this is anything other than {'': 0}\n",
    "# then probably some parts of the model got offloaded onto CPU and so will run slow.\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:51:52.338078Z",
     "iopub.status.busy": "2024-04-10T11:51:52.337697Z",
     "iopub.status.idle": "2024-04-10T11:51:54.262038Z",
     "shell.execute_reply": "2024-04-10T11:51:54.261131Z",
     "shell.execute_reply.started": "2024-04-10T11:51:52.338046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Expression: 2 + 2. Result: 4\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/2\n",
      "First, we add 2 and 2 together: 2 + 2 = 4\n",
      "\n",
      "Step 2/2\n",
      "Therefore, the expression 2 + 2 has a result of 4.<eos>\n",
      "CPU times: user 1.92 s, sys: 13.5 ms, total: 1.93 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = '''Expression: 2 + 2. Result:'''\n",
    "model_out = model.generate(\n",
    "    **tokenizer(doc, return_tensors='pt').to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:19:19.134297Z",
     "iopub.status.busy": "2024-04-10T12:19:19.133332Z",
     "iopub.status.idle": "2024-04-10T12:19:19.141054Z",
     "shell.execute_reply": "2024-04-10T12:19:19.140057Z",
     "shell.execute_reply.started": "2024-04-10T12:19:19.134267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\n",
      "\n",
      "Explain why the sky is blue<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"You are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\"\"\"\n",
    "task = \"\"\"Explain why the sky is blue\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{role}\\n\\n{task}\",\n",
    "    },\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.batch_decode(tokenized_chat)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:27:41.731154Z",
     "iopub.status.busy": "2024-04-10T12:27:41.730743Z",
     "iopub.status.idle": "2024-04-10T12:27:41.744498Z",
     "shell.execute_reply": "2024-04-10T12:27:41.743611Z",
     "shell.execute_reply.started": "2024-04-10T12:27:41.731112Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "docstrings = {}\n",
    "for name, obj in inspect.getmembers(torch.nn):\n",
    "    if inspect.isfunction(obj) or inspect.isclass(obj):\n",
    "        docstrings[name] = inspect.getdoc(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:32:28.495755Z",
     "iopub.status.busy": "2024-04-10T12:32:28.495165Z",
     "iopub.status.idle": "2024-04-10T12:32:28.501559Z",
     "shell.execute_reply": "2024-04-10T12:32:28.500642Z",
     "shell.execute_reply.started": "2024-04-10T12:32:28.495725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AdaptiveAvgPool1d', 'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d', 'AdaptiveLogSoftmaxWithLoss', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AlphaDropout', 'AvgPool1d', 'AvgPool2d', 'AvgPool3d', 'BCELoss', 'BCEWithLogitsLoss', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'Bilinear', 'CELU', 'CTCLoss', 'ChannelShuffle', 'CircularPad1d', 'CircularPad2d', 'CircularPad3d', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d', 'Container', 'Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d', 'CosineEmbeddingLoss', 'CosineSimilarity', 'CrossEntropyLoss', 'CrossMapLRN2d', 'DataParallel', 'Dropout', 'Dropout1d', 'Dropout2d', 'Dropout3d', 'ELU', 'Embedding', 'EmbeddingBag', 'FeatureAlphaDropout', 'Flatten', 'Fold', 'FractionalMaxPool2d', 'FractionalMaxPool3d', 'GELU', 'GLU', 'GRU', 'GRUCell', 'GaussianNLLLoss', 'GroupNorm', 'Hardshrink', 'Hardsigmoid', 'Hardswish', 'Hardtanh', 'HingeEmbeddingLoss', 'HuberLoss', 'Identity', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'KLDivLoss', 'L1Loss', 'LPPool1d', 'LPPool2d', 'LSTM', 'LSTMCell', 'LayerNorm', 'LazyBatchNorm1d', 'LazyBatchNorm2d', 'LazyBatchNorm3d', 'LazyConv1d', 'LazyConv2d', 'LazyConv3d', 'LazyConvTranspose1d', 'LazyConvTranspose2d', 'LazyConvTranspose3d', 'LazyInstanceNorm1d', 'LazyInstanceNorm2d', 'LazyInstanceNorm3d', 'LazyLinear', 'LeakyReLU', 'Linear', 'LocalResponseNorm', 'LogSigmoid', 'LogSoftmax', 'MSELoss', 'MarginRankingLoss', 'MaxPool1d', 'MaxPool2d', 'MaxPool3d', 'MaxUnpool1d', 'MaxUnpool2d', 'MaxUnpool3d', 'Mish', 'Module', 'ModuleDict', 'ModuleList', 'MultiLabelMarginLoss', 'MultiLabelSoftMarginLoss', 'MultiMarginLoss', 'MultiheadAttention', 'NLLLoss', 'NLLLoss2d', 'PReLU', 'PairwiseDistance', 'Parameter', 'ParameterDict', 'ParameterList', 'PixelShuffle', 'PixelUnshuffle', 'PoissonNLLLoss', 'RNN', 'RNNBase', 'RNNCell', 'RNNCellBase', 'RReLU', 'ReLU', 'ReLU6', 'ReflectionPad1d', 'ReflectionPad2d', 'ReflectionPad3d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'SELU', 'Sequential', 'SiLU', 'Sigmoid', 'SmoothL1Loss', 'SoftMarginLoss', 'Softmax', 'Softmax2d', 'Softmin', 'Softplus', 'Softshrink', 'Softsign', 'SyncBatchNorm', 'Tanh', 'Tanhshrink', 'Threshold', 'Transformer', 'TransformerDecoder', 'TransformerDecoderLayer', 'TransformerEncoder', 'TransformerEncoderLayer', 'TripletMarginLoss', 'TripletMarginWithDistanceLoss', 'Unflatten', 'Unfold', 'UninitializedBuffer', 'UninitializedParameter', 'Upsample', 'UpsamplingBilinear2d', 'UpsamplingNearest2d', 'ZeroPad1d', 'ZeroPad2d', 'ZeroPad3d', 'factory_kwargs'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstrings.keys()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelInstanceId": 6216,
     "sourceId": 11384,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 22003,
     "sourceId": 26140,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
