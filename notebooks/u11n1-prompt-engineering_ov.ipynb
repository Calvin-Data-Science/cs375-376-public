{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-09T21:07:34.746996Z",
     "iopub.status.busy": "2024-04-09T21:07:34.746521Z",
     "iopub.status.idle": "2024-04-09T21:07:52.907046Z",
     "shell.execute_reply": "2024-04-09T21:07:52.906074Z",
     "shell.execute_reply.started": "2024-04-09T21:07:34.746967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the environment\n",
    "#!pip install --upgrade huggingface_hub transformers\n",
    "#!pip install bitsandbytes\n",
    "#from huggingface_hub import login\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "#access_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "#login(token = access_token_read)\n",
    "#!pip install git+https://github.com/huggingface/transformers -U\n",
    "#!pip install accelerate\n",
    "#!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-10T12:06:11.539116Z",
     "iopub.status.busy": "2024-04-10T12:06:11.538756Z",
     "iopub.status.idle": "2024-04-10T12:07:28.563453Z",
     "shell.execute_reply": "2024-04-10T12:07:28.562619Z",
     "shell.execute_reply.started": "2024-04-10T12:06:11.539047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: loading model weights from the Internet. This might take a bit of extra time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:496: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1123: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3994a39916b489aa57cc46255b3022b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.2.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/transformers/models/gemma/modeling_gemma.py:982: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/Users/ka37/Courses/cs344/.venv/lib/python3.11/site-packages/nncf/torch/dynamic_graph/wrappers.py:80: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (127 / 127)            │ 100% (127 / 127)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24881d28b751455db1d5f8636c86622e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n",
    "\n",
    "# Work around a bug in the version of PyTorch and GPU hardware curretnly on Kaggle. On other hardware, removing these lines may lead to a speed-up.\n",
    "#torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "#torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Create the quantization configuration with desired quantization parameters\n",
    "q_config = OVWeightQuantizationConfig()#bits=8, group_size=128)#, ratio=0.8)\n",
    "\n",
    "# Create OpenVINO configuration with optimal settings for this model\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"CACHE_DIR\": \"model_cache\", \"INFERENCE_PRECISION_HINT\": \"f32\"}\n",
    "\n",
    "\n",
    "# Load the model\n",
    "USE_INSTRUCTION_TUNED = True # we'll switch this to True partway through the lab\n",
    "if USE_INSTRUCTION_TUNED:\n",
    "    model_name = '/kaggle/input/gemma/transformers/1.1-2b-it/1'\n",
    "    if not os.path.exists(model_name):\n",
    "        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n",
    "        model_name = \"google/gemma-1.1-2b-it\"\n",
    "else:\n",
    "    model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n",
    "    if not os.path.exists(model_name):\n",
    "        print(\"Warning: loading model weights from the Internet. This might take a bit of extra time.\")\n",
    "        model_name = \"google/gemma-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = OVModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #device='gpu',\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    use_auth_token=True,\n",
    "    export=True, # export model to OpenVINO format: should be False if model already exported\n",
    "    quantization_config=q_config,\n",
    "    ov_config=ov_config,\n",
    ")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "# Silence a warning.\n",
    "tokenizer.decode([tokenizer.eos_token_id]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:42:53.091769Z",
     "iopub.status.busy": "2024-04-10T11:42:53.091403Z",
     "iopub.status.idle": "2024-04-10T11:42:53.097809Z",
     "shell.execute_reply": "2024-04-10T11:42:53.096895Z",
     "shell.execute_reply.started": "2024-04-10T11:42:53.091734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check where the whole model is loaded and what data type it's using.\n",
    "model.device#, model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T21:12:05.348612Z",
     "iopub.status.busy": "2024-04-09T21:12:05.348261Z",
     "iopub.status.idle": "2024-04-09T21:12:05.420171Z",
     "shell.execute_reply": "2024-04-09T21:12:05.419141Z",
     "shell.execute_reply.started": "2024-04-09T21:12:05.348571Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OVModelForCausalLM' object has no attribute 'hf_device_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check where parameters are loaded. If this is anything other than {'': 0}\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# then probably some parts of the model got offloaded onto CPU and so will run slow.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_device_map\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OVModelForCausalLM' object has no attribute 'hf_device_map'"
     ]
    }
   ],
   "source": [
    "# Check where parameters are loaded. If this is anything other than {'': 0}\n",
    "# then probably some parts of the model got offloaded onto CPU and so will run slow.\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T11:51:52.338078Z",
     "iopub.status.busy": "2024-04-10T11:51:52.337697Z",
     "iopub.status.idle": "2024-04-10T11:51:54.262038Z",
     "shell.execute_reply": "2024-04-10T11:51:54.261131Z",
     "shell.execute_reply.started": "2024-04-10T11:51:52.338046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Expression: 2 + 2. "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = '''Expression: 2 + 2. Result:'''\n",
    "model_out = model.generate(\n",
    "    **tokenizer(doc, return_tensors='pt').to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:19:19.134297Z",
     "iopub.status.busy": "2024-04-10T12:19:19.133332Z",
     "iopub.status.idle": "2024-04-10T12:19:19.141054Z",
     "shell.execute_reply": "2024-04-10T12:19:19.140057Z",
     "shell.execute_reply.started": "2024-04-10T12:19:19.134267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\n",
      "\n",
      "Explain why the sky is blue<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"You are a helpful 2nd-grade teacher. Help a 2nd grader to answer questions in a short and clear manner.\"\"\"\n",
    "task = \"\"\"Explain why the sky is blue\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{role}\\n\\n{task}\",\n",
    "    },\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.batch_decode(tokenized_chat)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:27:41.731154Z",
     "iopub.status.busy": "2024-04-10T12:27:41.730743Z",
     "iopub.status.idle": "2024-04-10T12:27:41.744498Z",
     "shell.execute_reply": "2024-04-10T12:27:41.743611Z",
     "shell.execute_reply.started": "2024-04-10T12:27:41.731112Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "docstrings = {}\n",
    "for name, obj in inspect.getmembers(torch.nn):\n",
    "    if inspect.isfunction(obj) or inspect.isclass(obj):\n",
    "        docstrings[name] = inspect.getdoc(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:32:28.495755Z",
     "iopub.status.busy": "2024-04-10T12:32:28.495165Z",
     "iopub.status.idle": "2024-04-10T12:32:28.501559Z",
     "shell.execute_reply": "2024-04-10T12:32:28.500642Z",
     "shell.execute_reply.started": "2024-04-10T12:32:28.495725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AdaptiveAvgPool1d', 'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d', 'AdaptiveLogSoftmaxWithLoss', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AlphaDropout', 'AvgPool1d', 'AvgPool2d', 'AvgPool3d', 'BCELoss', 'BCEWithLogitsLoss', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'Bilinear', 'CELU', 'CTCLoss', 'ChannelShuffle', 'CircularPad1d', 'CircularPad2d', 'CircularPad3d', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d', 'Container', 'Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d', 'CosineEmbeddingLoss', 'CosineSimilarity', 'CrossEntropyLoss', 'CrossMapLRN2d', 'DataParallel', 'Dropout', 'Dropout1d', 'Dropout2d', 'Dropout3d', 'ELU', 'Embedding', 'EmbeddingBag', 'FeatureAlphaDropout', 'Flatten', 'Fold', 'FractionalMaxPool2d', 'FractionalMaxPool3d', 'GELU', 'GLU', 'GRU', 'GRUCell', 'GaussianNLLLoss', 'GroupNorm', 'Hardshrink', 'Hardsigmoid', 'Hardswish', 'Hardtanh', 'HingeEmbeddingLoss', 'HuberLoss', 'Identity', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'KLDivLoss', 'L1Loss', 'LPPool1d', 'LPPool2d', 'LSTM', 'LSTMCell', 'LayerNorm', 'LazyBatchNorm1d', 'LazyBatchNorm2d', 'LazyBatchNorm3d', 'LazyConv1d', 'LazyConv2d', 'LazyConv3d', 'LazyConvTranspose1d', 'LazyConvTranspose2d', 'LazyConvTranspose3d', 'LazyInstanceNorm1d', 'LazyInstanceNorm2d', 'LazyInstanceNorm3d', 'LazyLinear', 'LeakyReLU', 'Linear', 'LocalResponseNorm', 'LogSigmoid', 'LogSoftmax', 'MSELoss', 'MarginRankingLoss', 'MaxPool1d', 'MaxPool2d', 'MaxPool3d', 'MaxUnpool1d', 'MaxUnpool2d', 'MaxUnpool3d', 'Mish', 'Module', 'ModuleDict', 'ModuleList', 'MultiLabelMarginLoss', 'MultiLabelSoftMarginLoss', 'MultiMarginLoss', 'MultiheadAttention', 'NLLLoss', 'NLLLoss2d', 'PReLU', 'PairwiseDistance', 'Parameter', 'ParameterDict', 'ParameterList', 'PixelShuffle', 'PixelUnshuffle', 'PoissonNLLLoss', 'RNN', 'RNNBase', 'RNNCell', 'RNNCellBase', 'RReLU', 'ReLU', 'ReLU6', 'ReflectionPad1d', 'ReflectionPad2d', 'ReflectionPad3d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'SELU', 'Sequential', 'SiLU', 'Sigmoid', 'SmoothL1Loss', 'SoftMarginLoss', 'Softmax', 'Softmax2d', 'Softmin', 'Softplus', 'Softshrink', 'Softsign', 'SyncBatchNorm', 'Tanh', 'Tanhshrink', 'Threshold', 'Transformer', 'TransformerDecoder', 'TransformerDecoderLayer', 'TransformerEncoder', 'TransformerEncoderLayer', 'TripletMarginLoss', 'TripletMarginWithDistanceLoss', 'Unflatten', 'Unfold', 'UninitializedBuffer', 'UninitializedParameter', 'Upsample', 'UpsamplingBilinear2d', 'UpsamplingNearest2d', 'ZeroPad1d', 'ZeroPad2d', 'ZeroPad3d', 'factory_kwargs'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstrings.keys()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelInstanceId": 6216,
     "sourceId": 11384,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 22003,
     "sourceId": 26140,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
